{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset from the one of the following links:\n",
    "- https://www.nist.gov/itl/iad/image-group/emnist-dataset\n",
    "- https://www.kaggle.com/crawford/emnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Balanced dataset\n",
    "- Includes digits 0-9 and alphabet A-Za-z (where lower-case letters that are similar to upper-case letters are ommitted due to mis-classification)\n",
    "- Each digit and letters have equal number of samples\n",
    "- label is in the first column of loaded dataset and is corresponds to the order <br>\n",
    "    eg. letter A is labeled as 10 etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](dataset_info.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](dataset_info2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "letters_train_path = 'data/emnist-balanced-train.csv'\n",
    "letters_test_path = 'data/emnist-balanced-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(letters_train_path, header=None)\n",
    "test_data = pd.read_csv(letters_test_path, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change from panda dataframe to numpy array \n",
    "data_train = train_data.values\n",
    "data_test = test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112800, 785), (18800, 785))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape, data_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train: 112,800 <br>\n",
    "test: 18,800 <br>\n",
    "total: 131,600 <br>\n",
    "classes: 47 (from 0-9A-Za-z but 15 lower-alphabets are missing since they are omitted to avoid mis-classification errors) <br>\n",
    "image size: 28 by 28 (784) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Separate y value (which in first column)\n",
    "- One-Hot encode y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 112800 entries, 0 to 112799\n",
      "Columns: 785 entries, 0 to 784\n",
      "dtypes: int64(785)\n",
      "memory usage: 675.6 MB\n"
     ]
    }
   ],
   "source": [
    "# notice train_data is panda frame and data_train is numpy array \n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the datatype and separate the y values\n",
    "x_train = data_train[:, 1:]\n",
    "y_train = data_train[:, 0:1]\n",
    "\n",
    "x_test = data_test[:, 1:]\n",
    "y_test = data_test[:, 0:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encoding\n",
    "Turn y values to one hot encoding of [examples *47]\n",
    "- notice: y_train, y_test is a column vector. Function requires row vector to create a One-hot encoding \n",
    "- https://stackoverflow.com/questions/18691084/what-does-1-mean-in-numpy-reshape\n",
    "- https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encoder(data):\n",
    "    data = data.reshape(-1) # turn to row vector \n",
    "    hot_encoded = np.zeros((data.size,data.max()+1)) # create 0's of [ n * num_classes ]\n",
    "    hot_encoded[np.arange(data.size), data] = 1\n",
    "\n",
    "    return hot_encoded\n",
    "    \n",
    "y_train_hot = hot_encoder(y_train)\n",
    "y_test_hot = hot_encoder(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112800, 47), (18800, 47))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot.shape, y_test_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), array([26]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check to see one sample\n",
    "y_test_hot[3],y_test[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the data type from int to float\n",
    "x_train = x_train.astype('float32') \n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "some configuration for later use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of pixels in each dimension of an image.\n",
    "img_size = 28\n",
    "\n",
    "# The images are stored in one-dimensional arrays of this length.\n",
    "img_size_flat = x_train.shape[1] # 28 * 28 = 784\n",
    "\n",
    "# Tuple with height and width of images used to reshape arrays.\n",
    "img_shape = (img_size,img_size)\n",
    "\n",
    "# Number of classes, one class for each of characters.\n",
    "num_classes = 47 \n",
    "\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rotation\n",
    "for some reason, the image is flipped and rotated, thus we need to fix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try display one letter \n",
    "plt.imshow(x_test[3].reshape([28, 28]), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate(image):\n",
    "    image = image.reshape(img_shape) # put back shape to 28 by 28\n",
    "    image = np.fliplr(image)\n",
    "    image = np.rot90(image)\n",
    "    return image.reshape(img_size * img_size)\n",
    "\n",
    "# since it is a gray scale image \n",
    "# applying per row \n",
    "x_train = np.apply_along_axis(rotate, 1, x_train)\n",
    "x_test = np.apply_along_axis(rotate, 1, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(x_test[3].reshape(img_shape), cmap='binary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot some more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displays 9 images from the dataset \n",
    "def plot_images(images, cls_true, cls_pred=None):\n",
    "    assert len(images) == len(cls_true) == 9\n",
    "    \n",
    "    # Create figure with 3x3 sub-plots.\n",
    "    fig, axes = plt.subplots(3, 3)\n",
    "    fig.subplots_adjust(hspace=0.3, wspace=0.3)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        # Plot image.\n",
    "        ax.imshow(images[i].reshape(img_shape), cmap='binary')\n",
    "\n",
    "        # Show true and predicted classes.\n",
    "        if cls_pred is None:\n",
    "            xlabel = \"True: {0}\".format(cls_true[i])\n",
    "        else:\n",
    "            xlabel = \"True: {0}, Pred: {1}\".format(cls_true[i], cls_pred[i])\n",
    "\n",
    "        # Show the classes as the label on the x-axis.\n",
    "        ax.set_xlabel(xlabel)\n",
    "        \n",
    "        # Remove ticks from the plot.\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first 9 images from the train-set.\n",
    "images = x_train[0:9]\n",
    "\n",
    "# Get the true classes for those images.\n",
    "cls_true =y_train[0:9]\n",
    "\n",
    "# Plot the images and labels using our helper-function above.\n",
    "plot_images(images=images, cls_true=cls_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "- Since its image, max pixel is 255 thus we can just devide by 255 to get its range in [0,1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(255.0, 0.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0].max(),x_train[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_norm = x_train/255.\n",
    "x_test_norm = x_test/255.\n",
    "x_train_norm[0].max(),x_train_norm[0].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Train, Test, Validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we don't need to shuffle the data since data is not sorted \n",
    "# we will take 20 % of data from 112800 samples in train set\n",
    "num_valid = int(x_train_norm.shape[0] * 0.2)\n",
    "\n",
    "x_train = x_train_norm[num_valid:]\n",
    "y_train_hot = y_train_hot[num_valid:,:]\n",
    "\n",
    "x_valid = x_train_norm[:num_valid]\n",
    "y_valid_hot = y_train_hot[:num_valid,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90240, 784), (22560, 784), (18800, 784))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_valid.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((90240, 47), (22560, 47), (18800, 47))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_hot.shape, y_valid_hot.shape, y_test_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hot = y_train_hot.astype('float32') \n",
    "y_valid_hot = y_valid_hot.astype('float32') \n",
    "y_test_hot = y_test_hot.astype('float32') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future to do \n",
    "- Data Augmentation (shift, rotate, rescale) for getting more data to get higher accuracy\n",
    "- https://github.com/ChaitanyaBaweja/RotNIST/blob/master/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model architecture "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 Conv layer followed like:\n",
    "\n",
    "Conv -> activation -> max pooling -> Conv -> activation -> max pooling -> Flatten -> FCN -> FCN \n",
    "\n",
    "train on K-fold using scikit learn \n",
    "https://www.kaggle.com/wenyangsama/kfold-cnn-maxpooling\n",
    "https://chrisalbon.com/deep_learning/keras/k-fold_cross-validating_neural_networks/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration of Neural Network\n",
    "- https://medium.com/@jonathan_hui/improve-deep-learning-models-performance-network-tuning-part-6-29bf90df6d2d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Available Options to Try Out:  <br> \n",
    "__• Activation function__: \n",
    "    - 'relu':\n",
    "        - Default \n",
    "    - 'lrelu' (leaky relu):\n",
    "    - 'elu':\n",
    "        \n",
    "         \n",
    "__• Weight initialization__(default is None):\n",
    "    - 'xavier'\n",
    "        - preferred to be used when using tanh/sigmoid \n",
    "    - 'he'\n",
    "        - preferred to be used when using Relu \n",
    "    \n",
    "__• learning rate schedule__(default: None, None):\n",
    "    - types:\n",
    "        - 'constant': (Predetermined piecewise constant learning rate) \n",
    "        - 'performance': (Performance scheduling) \n",
    "        - 'exp': (Exponential Scheduling) \n",
    "        - 'pow': (Power scheduling) \n",
    "    - N epochs (default is 50):\n",
    "        - change learning rate every N epochs with specified schedule.\n",
    "\n",
    "__• Gradient Optimizer__(defualt: SDG):\n",
    "    - 'sdg'\n",
    "    - 'nestrov'\n",
    "    - 'momentum'\n",
    "    - 'adagrad' (not in Neural \n",
    "    - 'rmsprop' \n",
    "    - 'adam'\n",
    "\n",
    "__• Normalization__(defualt: None): \n",
    "Normalization will be applied after activation layer based on the discussion: https://www.reddit.com/r/MachineLearning/comments/67gonq/d_batch_normalization_before_or_after_relu/ <br>\n",
    "if you are curiouse about other normalization then check this post: http://yeephycho.github.io/2016/08/03/Normalizations-in-neural-networks/\n",
    "    - 'bn' (batch normalization)\n",
    "    - 'ln' (layer normalization)\n",
    "    - 'lrn' (Local Response Normalization)\n",
    "    \n",
    "__•Regularization__ (defualt: L2):\n",
    "    - 'l2'\n",
    "    - 'l1'\n",
    "    - 'dropout'\n",
    "    - 'maxout'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convolutional Layer 1.\n",
    "filter_size1 = 5          # kernal size 5 x 5 x 1\n",
    "num_filters1 = 16         # There are 16 of these filters.\n",
    "\n",
    "# Convolutional Layer 2.\n",
    "filter_size2 = 5          # kernal size 5 x 5 x 1\n",
    "num_filters2 = 36         # There are 36 of these filters.\n",
    "\n",
    "\n",
    "# Fully-connected layer.\n",
    "fc_size = 128             # Number of neurons in fully-connected layer.\n",
    "\n",
    "\n",
    "options = {\n",
    "        'act_func' : 'elu',\n",
    "        'weight_init' : 'xavier',\n",
    "}\n",
    "\n",
    "opt = 'nestrov'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear old variables\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_weights(wt_initializer=None):\n",
    "    if wt_initializer == \"Xavier\":\n",
    "        return tf.contrib.layers.variance_scaling_initializer(uniform=False)\n",
    "    elif wt_initializer == \"He\":\n",
    "        return tf.contrib.layers.xavier_initializer(uniform=False)\n",
    "    else:\n",
    "        return tf.glorot_uniform_initializer() # will use their default initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_biases(length):\n",
    "    return tf.Variable(tf.constant(0.01,shape=[length]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_conv_layer(input,              # The previous layer.\n",
    "                   num_input_channels, # Num. channels in prev. layer.\n",
    "                   filter_size,        # Width and height of each filter.\n",
    "                   num_filters,        # Number of kernel filters.\n",
    "                   options,            # Dictionary containing various options (weights, activation function)\n",
    "                   use_bn = True,      # Batch norm\n",
    "                   use_dropout=True,   # Use dropout\n",
    "                   keep_prob=0.9,      # Probability to keep for dropout\n",
    "                   use_pooling=True,   # Use 2x2 max-pooling.\n",
    "                   is_train=False):    # Training or not  \n",
    "    \n",
    "    # Shape of the filter-weights for the convolution.\n",
    "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
    "\n",
    "    # Create new weights aka. filters with the given shape.\n",
    "    weights = new_weights(wt_initializer=options.get('weight_init', None))\n",
    "\n",
    "    # Create new biases, one for each filter.\n",
    "    biases = new_biases(length=num_filters)\n",
    "    \n",
    "    kernel = tf.get_variable(\n",
    "        initializer=weights, \n",
    "        shape=shape,\n",
    "        name='kernel')\n",
    "\n",
    "    \n",
    "    layer = tf.nn.conv2d(input=input,\n",
    "                         filter=kernel,\n",
    "                         strides=[1, 1, 1, 1], # strides of 1\n",
    "                         padding='SAME')\n",
    "    \n",
    "    # Add the biases to the results of the convolution.\n",
    "    # A bias-value is added to each filter-channel.\n",
    "    layer += biases # [num_images, img_height, img_width, num_channels=num_filters]\n",
    "    \n",
    "    # Perform batch normaliazation\n",
    "    # code sampled from:\n",
    "    # - https://stackoverflow.com/questions/33949786/how-could-i-use-batch-normalization-in-tensorflow/33950177#33950177\n",
    "    # - https://gist.github.com/tomokishii/0ce3bdac1588b5cca9fa5fbdf6e1c412\n",
    "    # - https://github.com/davidslac/mlearntut/blob/master/BatchNormalization.py\n",
    "    \n",
    "    if use_bn:\n",
    "        beta = tf.Variable(tf.constant(0.0, shape=[num_filters]),\n",
    "                                     name='beta', trainable=True)\n",
    "        gamma = tf.Variable(tf.constant(1.0, shape=[num_filters]),\n",
    "                                      name='gamma', trainable=True)\n",
    "        # Calculate batch mean and variance\n",
    "        batch_mean, batch_var = tf.nn.moments(layer, [0,1,2], name='moments')\n",
    "        \n",
    "        # Create an ExponentialMovingAverage object\n",
    "        ema = tf.train.ExponentialMovingAverage(decay=0.5)\n",
    "\n",
    "        # To understand more detailes on EMA and how to use it, read this post: http://ruishu.io/2017/11/22/ema/\n",
    "        def mean_var_with_update():\n",
    "            ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "            with tf.control_dependencies([ema_apply_op]):\n",
    "                return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "                \n",
    "        # if train\n",
    "        # if test: Test time batch norm using learned gamma/beta and calculated running mean/var.\n",
    "        mean, var = tf.cond(is_train,\n",
    "                            mean_var_with_update,\n",
    "                            lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "        \n",
    "        normed = tf.nn.batch_normalization(layer, mean, var, beta, gamma, 1e-3)\n",
    "\n",
    "\n",
    "    # I decided to do pooling before activation function (although for max pool it doesn't matter the order)\n",
    "    # Use pooling to down-sample the image resolution?\n",
    "    if use_pooling:\n",
    "        # This is 2x2 max-pooling\n",
    "        layer = tf.nn.max_pool(value=layer,\n",
    "                               ksize=[1, 2, 2, 1], # [batch_size, kernel_size, kernel_size, channels]\n",
    "                               strides=[1, 2, 2, 1],\n",
    "                               padding='SAME')\n",
    "\n",
    "        \n",
    "    # Perform activation function of choice\n",
    "    act = options.get('act_func','relu')\n",
    "    if act == 'relu':    \n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif act == 'lrelu':\n",
    "        layer = tf.nn.leaky_relu(layer, alpha=0.2)\n",
    "    elif act == 'elu':\n",
    "        layer = tf.nn.elu(layer)\n",
    "        \n",
    "    # applying dropout in conv layer just to see how it reacts. Set it low. \n",
    "    # https://stats.stackexchange.com/questions/240305/where-should-i-place-dropout-layers-in-a-neural-network\n",
    "    if use_dropout:\n",
    "        tf.nn.dropout(layer,keep_prob=0.9)\n",
    "        \n",
    "    return layer, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_layer(layer):\n",
    "    # Get the shape of the input layer.\n",
    "    layer_shape = layer.get_shape()\n",
    "\n",
    "    # The shape of the input layer is assumed to be:\n",
    "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
    "\n",
    "    # The number of features is: img_height * img_width * num_channels\n",
    "    num_features = layer_shape[1:4].num_elements()\n",
    "    \n",
    "    # Reshape the layer to [num_images, num_features].\n",
    "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
    "\n",
    "    # The shape of the flattened layer is now:\n",
    "    # [num_images, img_height * img_width * num_channels]\n",
    "\n",
    "    # Return both the flattened layer and the number of features.\n",
    "    return layer_flat, num_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_fc_layer(input,          # The previous layer.\n",
    "                 num_inputs,     # Num. inputs from prev. layer.\n",
    "                 num_outputs,    # Num. outputs.\n",
    "                 options,\n",
    "                 use_relu=True): # Use Rectified Linear Unit\n",
    "\n",
    "    # Create new weights and biases.\n",
    "    weights = tf.Variable(tf.truncated_normal(shape=[num_inputs, num_outputs], stddev=0.05))\n",
    "    biases = new_biases(length=num_outputs)\n",
    "\n",
    "    # Calculate the layer as the matrix multiplication of\n",
    "    # the input and weights, and then add the bias-values.\n",
    "    layer = tf.matmul(input, weights) + biases\n",
    "\n",
    "    \n",
    "    # Perform activation function of choice\n",
    "    act = options.get('act_func','relu')\n",
    "    if act == 'relu':    \n",
    "        layer = tf.nn.relu(layer)\n",
    "    elif act == 'lrelu':\n",
    "        layer = tf.nn.leaky_relu(layer, alpha=0.2)\n",
    "    elif act == 'elu':\n",
    "        layer = tf.nn.elu(layer)\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(x,y,keep_prob=0.9,is_train=False):\n",
    "    x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
    "            \n",
    "    ########################################################################################################\n",
    "    # conv layer 1\n",
    "    with tf.variable_scope('conv_1'):\n",
    "        layer_conv1, weights_conv1 = \\\n",
    "        new_conv_layer(input=x_image,\n",
    "                       num_input_channels=num_channels,\n",
    "                       filter_size=filter_size1,\n",
    "                       num_filters=num_filters1,\n",
    "                       options=options,\n",
    "                       use_bn = True,\n",
    "                       use_dropout = True,\n",
    "                       use_pooling=True,\n",
    "                       keep_prob=keep_prob,\n",
    "                       is_train=is_train)\n",
    "\n",
    "    ########################################################################################################\n",
    "    # conv layer 2\n",
    "    with tf.variable_scope('conv_2'):\n",
    "        layer_conv2, weights_conv2 = \\\n",
    "        new_conv_layer(input=layer_conv1,\n",
    "                       num_input_channels=num_filters1,\n",
    "                       filter_size=filter_size2,\n",
    "                       num_filters=num_filters2,\n",
    "                       options=options,\n",
    "                       use_bn = True,\n",
    "                       use_dropout = True,\n",
    "                       use_pooling=True,\n",
    "                       keep_prob=keep_prob,\n",
    "                       is_train=is_train)\n",
    "\n",
    "    ########################################################################################################\n",
    "    # flatten layer\n",
    "    layer_flat, num_features = flatten_layer(layer_conv2)\n",
    "\n",
    "    ########################################################################################################\n",
    "    # fully connected layer 1\n",
    "    with tf.variable_scope('fc1'):\n",
    "        layer_fc1 = new_fc_layer(input=layer_flat,\n",
    "                         num_inputs=num_features, \n",
    "                         num_outputs=fc_size, # 128,\n",
    "                         options=options,\n",
    "                         use_relu=True)\n",
    "    \n",
    "    ########################################################################################################\n",
    "    # fully connected layer 2\n",
    "    with tf.variable_scope('fc2'):\n",
    "        layer_fc2 = new_fc_layer(input=layer_fc1,\n",
    "                         num_inputs=fc_size, # 128 \n",
    "                         num_outputs=num_classes, # 47 \n",
    "                        options=options,\n",
    "                         use_relu=True)\n",
    "    \n",
    "    return layer_fc2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup input (e.g. the data that changes every batch)\n",
    "# The first dim is None, and gets sets automatically based on batch size fed in\n",
    "X = tf.placeholder(tf.float32, [None, img_size_flat])\n",
    "y = tf.placeholder(tf.float32, [None, num_classes],name='y_true') # contains truth\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32) # if using the dropout\n",
    "is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "y_true_cls = tf.argmax(y, axis=1)\n",
    "\n",
    "y_pred_logits = create_model(X, y, keep_prob, is_training)\n",
    "\n",
    "########################################################################################################\n",
    "# calculate accuracy \n",
    "y_pred = tf.nn.softmax(y_pred_logits) # turn to probability using softmax \n",
    "y_pred_cls = tf.argmax(y_pred, axis=1) # turn to a vector containing indicies of highest probability\n",
    "correct_prediction = tf.equal(y_pred_cls, y_true_cls) # contains 1s (meaning predicted correctly) and 0s (incorrect)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # calculate mean\n",
    "\n",
    "########################################################################################################\n",
    "# calculate loss\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_logits,\n",
    "                                                    labels=y)\n",
    "mean_loss = tf.reduce_mean(cross_entropy)\n",
    "##################################################################################################\n",
    "\n",
    "\n",
    "# Will be using Exponential Scheduling \n",
    "\n",
    "initial_lr = 0.1  # initiali lerning rate \n",
    "decay_steps = 1000 # every 1000 steps \n",
    "decay_rate = 1/10\n",
    "global_step = tf.Variable(0,trainable=False,name=\"global_step\")\n",
    "learning_rate = tf.train.exponential_decay(initial_lr,global_step,decay_steps,decay_rate)\n",
    "\n",
    "\n",
    "\n",
    "if opt == 'nestrov':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9,use_nesterov=True)\n",
    "elif opt == 'momentum':\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=0.9)\n",
    "elif opt == 'rmsprop':\n",
    "    optimizer = tf.tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "elif opt == 'adam':\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "else:\n",
    "    optimizer =  tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    \n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "with tf.control_dependencies(extra_update_ops):\n",
    "    traning_op = optimizer.minimize(mean_loss, global_step=global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(session, predict, Xd, yd, epochs=5001, batch_size=128,training=None):\n",
    "    \n",
    "    # shuffle indicies\n",
    "    train_indicies = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(train_indicies)\n",
    "    \n",
    "    training_now = training is not None\n",
    "    \n",
    "    # setting up variables we want to compute (and optimizing)\n",
    "    # if we have a training function, add that to things we compute\n",
    "    variables = [mean_loss, correct_prediction, accuracy]\n",
    "        \n",
    "    if training_now:\n",
    "        variables[-1] = training\n",
    "        \n",
    "    # counter \n",
    "    iter_cnt = 0\n",
    "    for i in range(epochs):\n",
    "        # keep track of losses and accuracy\n",
    "        correct = 0\n",
    "        losses = []\n",
    "        print()\n",
    "        for b in range(int(math.ceil(int(Xd.shape[0])/batch_size))):\n",
    "            \n",
    "            # generate indicies for the batch\n",
    "            start_idx = (b*batch_size)%Xd.shape[0]\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "           \n",
    "            # create a feed dictionary for this batch\n",
    "            feed_dict = {X: Xd[idx,:],\n",
    "                         y: yd[idx,:],\n",
    "                         keep_prob: 0.9,\n",
    "                         is_training: True,\n",
    "                         }\n",
    "\n",
    "            # get batch size\n",
    "            actual_batch_size = yd[idx].shape[0]\n",
    "\n",
    "            loss, corr, _ = session.run(variables,feed_dict=feed_dict)\n",
    "          \n",
    "            # aggregate performance stats\n",
    "            losses.append(loss*actual_batch_size)\n",
    "            correct += np.sum(corr)\n",
    "            \n",
    "            if training_now and (iter_cnt % 100) == 0:\n",
    "                print(\"Iteration {0}: with minibatch training loss = {1:.3g} and accuracy of {2:.2g}\"\\\n",
    "                          .format(iter_cnt,loss,np.sum(corr)/actual_batch_size))\n",
    "            iter_cnt += 1\n",
    "        total_correct = correct/Xd.shape[0]\n",
    "        total_loss = np.sum(losses)/Xd.shape[0]\n",
    "        \n",
    "        print(\"Epoch {2}, Overall loss = {0:.3g} and accuracy of {1:.3g}\"\\\n",
    "              .format(total_loss,total_correct,i+1))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "\n",
      "Iteration 0: with minibatch training loss = 3.84 and accuracy of 0.031\n",
      "Iteration 100: with minibatch training loss = 1.19 and accuracy of 0.67\n",
      "Iteration 200: with minibatch training loss = 0.503 and accuracy of 0.86\n",
      "Iteration 300: with minibatch training loss = 0.944 and accuracy of 0.72\n",
      "Iteration 400: with minibatch training loss = 0.543 and accuracy of 0.8\n",
      "Iteration 500: with minibatch training loss = 0.481 and accuracy of 0.81\n",
      "Iteration 600: with minibatch training loss = 0.425 and accuracy of 0.91\n",
      "Iteration 700: with minibatch training loss = 0.591 and accuracy of 0.75\n",
      "Iteration 800: with minibatch training loss = 0.715 and accuracy of 0.77\n",
      "Iteration 900: with minibatch training loss = 0.362 and accuracy of 0.88\n",
      "Iteration 1000: with minibatch training loss = 0.355 and accuracy of 0.88\n",
      "Iteration 1100: with minibatch training loss = 0.357 and accuracy of 0.89\n",
      "Iteration 1200: with minibatch training loss = 0.489 and accuracy of 0.83\n",
      "Iteration 1300: with minibatch training loss = 0.462 and accuracy of 0.91\n",
      "Iteration 1400: with minibatch training loss = 0.481 and accuracy of 0.91\n",
      "Epoch 1, Overall loss = 0.601 and accuracy of 0.807\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    print(\"Training\")\n",
    "    run_model(sess,y_pred,x_train,y_train_hot,1,64,traning_op)\n",
    "    \n",
    "#     print(\"Validation\")\n",
    "#     run_model(sess,y_pred,loss,x_val,y_val_hot,1,64)\n",
    "    \n",
    "#     print('Test')\n",
    "#     run_model(sess,y_out,mean_loss,x_test,y_test,1,64)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NLP]",
   "language": "python",
   "name": "conda-env-NLP-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
